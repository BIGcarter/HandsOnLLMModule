{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力\n",
    "## Multihead Attention (MHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super.__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size//num_heads\n",
    "        # 注意力模块需要Wq Wk Wv变换层，输出变换层，dropout\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)   # Wq\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)   # Wk\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)   # Wv\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)   # Wo\n",
    "        self.dropout = 0.0\n",
    "        self.attn_dropout = nn.Dropout(self.dropout)\n",
    "        self.resid_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.FlashAttention = hasattr(F, 'scaled_dot_product_attention')  # 判断torch版本有无flashattention2\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: bool = True) -> torch.Tensor:\n",
    "        batch_size, seq_len, hidden_size = hidden_state.size()\n",
    "        q = self.q_linear(hidden_state) # (batch_size, seq_len, hidden_size)\n",
    "        k = self.k_linear(hidden_state)\n",
    "        v = self.v_linear(hidden_state)\n",
    "        #1  (batch_size, seq_len, hidden_size) -> (batch_size, seq_len, num_heads, hidden_size//num_heads) -> (batch_size, num_heads, seq_len, hidden_size//num_heads)\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2) \n",
    "        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2) \n",
    "        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2) \n",
    "        if self.FlashAttention:  # 若有flashattention2则调之，无则手动实现MHA\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=attention_mask, dropout_p = self.dropout if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            attn = q @ k.transpose(-1, -2) / torch.sqrt(torch.Tensor(self.head_dim))\n",
    "            casual_mask = nn.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "            attn = attn.masked_fill(casual_mask, float('-inf'))\n",
    "            attn = F.softmax(attn)\n",
    "            attn = self.attn_dropout(attn)\n",
    "            y = attn @ v\n",
    "\n",
    "        #2 (batch_size, seq_len, num_heads, hidden_size//num_heads) -> (batch_size, seq_len, hidden_size)\n",
    "        # #2为#1的相反变换过程\n",
    "        # view()之前需要先用contiguous()使得内存连续\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, seq_len, hidden_size)\n",
    "        # 注意力结果经过一个线性变换层再dropout（optional）\n",
    "        y = self.resid_dropout(self.o_linear(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多查询注意力\n",
    "## MultiQuery Attention (MQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里我忽略掉dropout\n",
    "# 具体处理参考上面MHA实现\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super.__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        # \n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, self.head_dim)\n",
    "        self.v_linear = nn.Linear(hidden_size, self.head_dim)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.FlashAttention = hasattr(F, 'scaled_dot_product_attention')\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: bool = True):\n",
    "        batch_size, seq_len, hidden_size = hidden_state.size()\n",
    "        dk = 1/torch.sqrt(self.head_dim)\n",
    "        q = self.q_linear(hidden_state)\n",
    "        k = self.k_linear(hidden_state)\n",
    "        v = self.v_linear(hidden_state)\n",
    "        # 每一个注意力模块共享一个k，v，将q分头，view将qkv的形状对齐\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_size, -1, 1, self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size, -1, 1, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # @算符,torch.mulmat, F.scaled_dot_product_attention能通过广播机制方便地实现MHA\n",
    "        if self.FlashAttention:\n",
    "            y = F.scaled_dot_product_attention(q, k.transpose(-1,-2), dropout_p = 0.0, is_causal=attention_mask)\n",
    "        else:\n",
    "            attn = q @ k.transpose(-1,-2) * dk\n",
    "            casual_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "            attn = attn.fill_masked(casual_mask, float('-inf'))\n",
    "            attn = F.softmax(attn)\n",
    "            y = attn @ v\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, seq_len, hidden_size)\n",
    "        y = self.o_linear(y)\n",
    "        return y\n",
    "            \n",
    "               \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 组查询注意力\n",
    "## Group Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, num_groups):\n",
    "        super.__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        assert hidden_size % num_groups == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, self.num_groups * self.head_dim)\n",
    "        self.v_linear = nn.Linear(hidden_size, self.num_groups * self.head_dim)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.FlashAttention = hasattr(F, 'scaled_dot_product_attention')\n",
    "    \n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: bool = True):\n",
    "        batch_size, seq_len, hidden_size = hidden_state.size()\n",
    "        dk = 1/nn.sqrt(self.head_dim)\n",
    "        q = self.q_linaer(hidden_state)\n",
    "        k = self.k_linaer(hidden_state)\n",
    "        v = self.v_linaer(hidden_state)\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_size, -1, self.num_groups, self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size, -1, self.num_groups, self.head_dim).transpose(1,2)\n",
    "        k = k[:, :, None, :, :].expand(batch_size, self.num_groups, self.num_heads//self.num_groups, seq_len, self.head_dim)\n",
    "        v = v[:, :, None, :, :].expand(batch_size, self.num_groups, self.num_heads//self.num_groups, seq_len, self.head_dim)\n",
    "        # GQA的难点在这里。\n",
    "        # k[:,:,None,:,:]可以新增一个维度，[batch_size, num_groups, seq_len, head_dim] -> [batch_size, num_groups, 1, seq_len, head_dim]\n",
    "        # expand可以“复制”后面维度的数据成self.num_heads//self.num_groups份。不是真正的复制，而是只返回view，数据依旧共享内存。\n",
    "\n",
    "        if self.FlashAttention:\n",
    "            y = F.scaled_dot_product_attention(q, k.transpose(-1,-2), v, dropout_p=0.0, is_causal=attention_mask)\n",
    "        else:\n",
    "            attn = q @ k.transpose(-1,-2) * dk\n",
    "            casual_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1,1,seq_len, seq_len)\n",
    "            attn = attn.fill_masked(casual_mask, float('-inf'))\n",
    "            attn = F.softmax(attn)\n",
    "            y = attn @ v\n",
    "        \n",
    "        y = self.o_linear(y)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_freqs_cis(dim: int, seqlen:int, base:float=10000.0) -> torch.Tensor:\n",
    "    ## m和theta求外积，转成向量，1为幅值，m_theta为指数\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, dim, 2)/dim))\n",
    "    m = torch.arange(seqlen)\n",
    "    m_theta = torch.outer(m, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(m_theta), m_theta)\n",
    "    return freqs_cis\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x:torch.Tensor) -> torch.Tensor:\n",
    "    ## 将旋转矩阵的形状变换成qk矩阵的形状，以供下一步广播\n",
    "    ndim = x.ndim\n",
    "    assert freqs_cis.shape==(x.shape[1], x.shape[-1]), \"matrixs dont match.\"\n",
    "    freq_shape = [d if i==1 or i==ndim-1 else 1 for i,d in enumerate(x.shape)]  # [seqlen, dim//2] -> [1, seqlen, 1, dim//2]\n",
    "    return freqs_cis.view(*freq_shape)\n",
    "\n",
    "def apply_rope(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_reshape = xq.reshape(xq.shape[:-1], -1, 2) # [batch_size, seq_len, num_heads, head_dim] -> [batch_size, seq_len, num_heads, head_dim//2, 2]??\n",
    "    xk_reshape = xk.reshape(xk.shape[:-1], -1, 2)\n",
    "    xq_complex = torch.view_as_complex(xq_reshape)\n",
    "    xk_complex = torch.view_as_complex(xk_reshape)\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_complex)\n",
    "    xq_out = torch.view_as_real(xq_complex*freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_complex*freqs_cis).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "# 下面是伪代码，只说明RoPE如何加入到注意力层中\n",
    "class Lite_attention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, seq_len):\n",
    "        super.__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size//num_heads\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)   # Wq\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)   # Wk\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)   # Wv\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)   # Wo\n",
    "        self.freqs_cis = compute_freqs_cis(self.head_dim, seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        self.xq = self.q_linear(x)\n",
    "        self.xk = self.k_linear(x)\n",
    "        self.xv = self.v_linear(x)\n",
    "        self.xq = self.xq.view(batch_size, seq_len, self.num_heads, -1)\n",
    "        self.xk = self.xk.view(batch_size, seq_len, self.num_heads, -1)\n",
    "        self.xv = self.xv.view(batch_size, seq_len, self.num_heads, -1)\n",
    "        self.xq_, self.xk_ = apply_rope(self.xq, self.xk, self.freqs_cis)\n",
    "        ##下面执行Attention计算....\n",
    "        ## Attention calulation......\n",
    "        ## \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention with KV Cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里继续忽略掉dropout\n",
    "class MultiHeadAttention_with_KVCache(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_size%num_heads == 0\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_cache = self.v_cache = None\n",
    "        self.total_len = 0\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q = self.q_linear(x)\n",
    "        k = self.k_linear(x)\n",
    "        v = self.v_linear(x)\n",
    "        q = q.view(batch_size, seq_len, -1, self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_size, seq_len, -1, self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size, seq_len, -1, self.head_dim).transpose(1,2)\n",
    "        ## KV Cache, 用内存换速度，内存大小(t+o)*hidden_size * 4byte (float32), t为prompt长度，o为已输出token长度\n",
    "        if self.k_cache == None:  # \n",
    "            self.k_cache = k\n",
    "            self.v_cache = v\n",
    "            self.total_len = seq_len\n",
    "        else:\n",
    "            self.k_cache = torch.cat([self.k_cache, k], dim = 2)\n",
    "            self.v_cache = torch.cat([self.v_cache, v], dim = 2)\n",
    "            k = self.k_cache\n",
    "            v = self.v_cache\n",
    "            self.total_len += seq_len\n",
    "        mask = nn.tril(torch.ones((self.total_len, self.total_len)).view(1,1,self.total_len,self.total_len))\n",
    "   \n",
    "        \n",
    "        atten = q@k.transpose(-1, -2) / torch.sqrt(self.head_dim)\n",
    "        atten = atten.mask_filled(mask, float('-inf'))\n",
    "        y = F.softmax(atten, dim=-1) @ v\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, seq_len, self.hidden_size)\n",
    "        return self.o_linear(y)\n",
    "    \n",
    "\n",
    "class MultiQueryAttention_with_KVCache(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.head_dim  = hidden_size // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(hidden_size)\n",
    "        self.k_linear = nn.Linear(self.head_dim)\n",
    "        self.v_linear = nn.Linear(self.head_dim)        \n",
    "        self.o_linear = nn.Linear(hidden_size)\n",
    "        self.k_cache = self.v_cache = None\n",
    "        self.total_len = 0\n",
    "\n",
    "    # 兼顾训练和推理时有无kvcache的情况\n",
    "    # 以及单token推理时应该可以去掉mask\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q, k, v = self.q_linear(x), self.k_linear(x), self.v_linear(x)\n",
    "        # 内存大小(t+o)*head_dim * 4byte (float32), t为prompt长度，o为已输出token长度\n",
    "        # 缩小num_heads倍\n",
    "        if self.training:\n",
    "            mask = torch.tril(torch.ones(self.seq_len, self.seq_len)).view(1,1,self.seq_len, self.seq_len)\n",
    "        else:\n",
    "            if self.k_cache == None:\n",
    "                self.k_cache, self.v_cache = k, v\n",
    "                self.total_len = seq_len\n",
    "                mask = torch.tril(torch.ones(self.seq_len, self.seq_len)).view(1,1,self.seq_len, self.seq_len)\n",
    "            else:\n",
    "                self.k_cache = torch.concat([self.k_cache, k])\n",
    "                self.v_cache = torch.concat([self.v_cache, v])\n",
    "                k = self.k_cache\n",
    "                v = self.v_cache\n",
    "                self.total_len += seq_len\n",
    "\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, -1).transpose(1,2)\n",
    "        k = k.view(batch_size, seq_len, 1, self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size, seq_len, 1, self.head_dim).transpose(1,2)\n",
    "\n",
    "        atten = q@k.transpose(-1,-2) / torch.sqrt(self.head_dim)\n",
    "        if self.training:\n",
    "            atten = atten.masked_fill(mask, float('-inf'))  # 在\n",
    "        atten = F.softmax(atten, dim=-1)\n",
    "        y = atten @ v\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, seq_len, self.hidden_size)\n",
    "        return self.o_linear(y)\n",
    "\n",
    "class GroupQueryAttention_with_KVCache(nn.Module):\n",
    "    def __init__(self, hidden_size, num_groups, num_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_size%num_heads==0 and hidden_size%num_groups==0\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.head_dim = hidden_size//self.num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, self.num_groups*self.head_dim)\n",
    "        self.v_linear = nn.Linear(hidden_size, self.num_groups*self.head_dim)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.total_len = 0\n",
    "        self.k_cache = self.v_cache = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q, k, v = self.q_linear(x), self.k_linear(x), self.v_linear(x)\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_size, -1, self.num_groups, self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size, -1, self.num_groups, self.head_dim).transpose(1,2)\n",
    "        k = k[:,:,None,:,:].expand(batch_size, self.num_groups, self.num_heads//self.num_groups, seq_len, self.head_dim)\n",
    "        v = v[:,:,None,:,:].expand(batch_size, self.num_groups, self.num_heads//self.num_groups, seq_len, self.head_dim)\n",
    "        if self.k_cache == None:\n",
    "            self.k_cache = k\n",
    "            self.v_cache = v\n",
    "            self.total_len = seq_len\n",
    "        else:\n",
    "            self.k_cache = torch.concat([self.k_cache, k], dim=3)\n",
    "            self.v_cache = torch.concat([self.v_cache, v], dim=3)\n",
    "            k = self.k_cache\n",
    "            v = self.v_cache\n",
    "            self.total_len += seq_len\n",
    "        \n",
    "        mask = torch.tril(torch.ones(self.total_len, self.total_len)).view(1, 1, self.total_len, self.total_len)\n",
    "        atten = q @ k.transpose(-2,-1) / torch.sqrt(self.head_dim)\n",
    "        atten = atten.masked_fill(mask==0, float('-inf'))\n",
    "        y = F.softmax(atten, dim = -1) @ v\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return self.o_lieanr(y)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dendro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
