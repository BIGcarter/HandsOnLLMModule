{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力\n",
    "## Multihead Attention (MHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super.__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size//num_heads\n",
    "        # 注意力模块需要Wq Wk Wv变换层，输出变换层，dropout\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)   # Wq\n",
    "        self.k_linear = nn.Linear(hidden_size, hidden_size)   # Wk\n",
    "        self.v_linear = nn.Linear(hidden_size, hidden_size)   # Wv\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)   # Wo\n",
    "        self.dropout = 0.0\n",
    "        self.attn_dropout = nn.Dropout(self.dropout)\n",
    "        self.resid_dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        self.FlashAttention = hasattr(F, 'scaled_dot_product_attention')  # 判断torch版本有无flashattention2\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: bool = True) -> torch.Tensor:\n",
    "        batch_size, seq_len, hidden_size = hidden_state.size()\n",
    "        q = self.q_linear(hidden_state) # (batch_size, seq_len, hidden_size)\n",
    "        k = self.k_linear(hidden_state)\n",
    "        v = self.v_linear(hidden_state)\n",
    "        #1  (batch_size, seq_len, hidden_size) -> (batch_size, seq_len, num_heads, hidden_size//num_heads) -> (batch_size, num_heads, seq_len, hidden_size//num_heads)\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2) \n",
    "        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2) \n",
    "        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2) \n",
    "        if self.FlashAttention:  # 若有flashattention2则调之，无则手动实现MHA\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=attention_mask, dropout_p = self.dropout if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            attn = q @ k.transpose(-1, -2) / torch.sqrt(torch.Tensor(self.head_dim))\n",
    "            casual_mask = nn.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "            attn = attn.masked_fill(casual_mask, float('-inf'))\n",
    "            attn = F.softmax(attn)\n",
    "            attn = self.attn_dropout(attn)\n",
    "            y = attn @ v\n",
    "\n",
    "        #2 (batch_size, seq_len, num_heads, hidden_size//num_heads) -> (batch_size, seq_len, hidden_size)\n",
    "        # #2为#1的相反变换过程\n",
    "        # view()之前需要先用contiguous()使得内存连续\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, seq_len, hidden_size)\n",
    "        # 注意力结果经过一个线性变换层再dropout（optional）\n",
    "        y = self.resid_dropout(self.o_linear(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多查询注意力\n",
    "## MultiQuery Attention (MQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里我忽略掉dropout\n",
    "# 具体处理参考上面MHA实现\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super.__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        # \n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, self.head_dim)\n",
    "        self.v_linear = nn.Linear(hidden_size, self.head_dim)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.FlashAttention = hasattr(F, 'scaled_dot_product_attention')\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: bool = True):\n",
    "        batch_size, seq_len, hidden_size = hidden_state.size()\n",
    "        dk = 1/torch.sqrt(self.head_dim)\n",
    "        q = self.q_linear(hidden_state)\n",
    "        k = self.k_linear(hidden_state)\n",
    "        v = self.v_linear(hidden_state)\n",
    "        # 每一个注意力模块共享一个k，v，将q分头，view将qkv的形状对齐\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_size, -1, 1, self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size, -1, 1, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # @算符,torch.mulmat, F.scaled_dot_product_attention能通过广播机制方便地实现MHA\n",
    "        if self.FlashAttention:\n",
    "            y = F.scaled_dot_product_attention(q, k.transpose(-1,-2), dropout_p = 0.0, is_causal=attention_mask)\n",
    "        else:\n",
    "            attn = q @ k.transpose(-1,-2) * dk\n",
    "            casual_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "            attn = attn.fill_masked(casual_mask, float('-inf'))\n",
    "            attn = F.softmax(attn)\n",
    "            y = attn @ v\n",
    "        y = y.transpose(1,2).contiguous().view(batch_size, seq_len, hidden_size)\n",
    "        y = self.o_linear(y)\n",
    "        return y\n",
    "            \n",
    "               \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 组查询注意力\n",
    "## Group Query Attention (GQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, num_groups):\n",
    "        super.__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        assert hidden_size % num_groups == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.num_groups = num_groups\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_linear = nn.Linear(hidden_size, self.num_groups * self.head_dim)\n",
    "        self.v_linear = nn.Linear(hidden_size, self.num_groups * self.head_dim)\n",
    "        self.o_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.FlashAttention = hasattr(F, 'scaled_dot_product_attention')\n",
    "    \n",
    "    def forward(self, hidden_state: torch.Tensor, attention_mask: bool = True):\n",
    "        batch_size, seq_len, hidden_size = hidden_state.size()\n",
    "        dk = 1/nn.sqrt(self.head_dim)\n",
    "        q = self.q_linaer(hidden_state)\n",
    "        k = self.k_linaer(hidden_state)\n",
    "        v = self.v_linaer(hidden_state)\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
    "        k = k.view(batch_size, -1, self.num_groups, self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size, -1, self.num_groups, self.head_dim).transpose(1,2)\n",
    "        k = k[:, :, None, :, :].expand(batch_size, self.num_groups, self.num_heads//self.num_groups, seq_len, self.head_dim)\n",
    "        v = v[:, :, None, :, :].expand(batch_size, self.num_groups, self.num_heads//self.num_groups, seq_len, self.head_dim)\n",
    "        # GQA的难点在这里。\n",
    "        # k[:,:,None,:,:]可以新增一个维度，[batch_size, num_groups, seq_len, head_dim] -> [batch_size, num_groups, 1, seq_len, head_dim]\n",
    "        # expand可以“复制”后面维度的数据成self.num_heads//self.num_groups份。不是真正的复制，而是只返回view，数据依旧共享内存。\n",
    "\n",
    "        if self.FlashAttention:\n",
    "            y = F.scaled_dot_product_attention(q, k.transpose(-1,-2), v, dropout_p=0.0, is_causal=attention_mask)\n",
    "        else:\n",
    "            attn = q @ k.transpose(-1,-2) * dk\n",
    "            casual_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1,1,seq_len, seq_len)\n",
    "            attn = attn.fill_masked(casual_mask, float('-inf'))\n",
    "            attn = F.softmax(attn)\n",
    "            y = attn @ v\n",
    "        \n",
    "        y = self.o_linear(y)\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dendro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
