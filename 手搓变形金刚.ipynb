{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手搓nanoGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 位置嵌入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int):   # d_model: 模型维度，词嵌入维度   max_len: 句子长度，序列长度\n",
    "        super.__init__()  # 初始化nn.Module\n",
    "        position_sentence = torch.arange(max_len).unsqueeze(1) \n",
    "        position_vec = torch.exp(torch.arange(0, d_model, 2)*(-torch.log(torch.tensor(10000.0))/d_model))  \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position_sentence*position_vec)\n",
    "        pe[:, 1::2] = torch.cos(position_sentence*position_vec)\n",
    "        self.register_buffer('pe', pe)  # 注册成缓存，不更新参数\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size[1]\n",
    "        x = x + self.pe[:seq_len]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自注意力模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super.__init__()\n",
    "        assert config.n_embd % config.n_head == 0 \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd, bias = config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias= config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.flash = hasattr(nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1,config.block_size,config.block_size))\n",
    "    \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2)  # (B,T,nh, hs) -> (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scale_dot_product_attention(q,k,v, attn_mask = None, dropout_p = self.dropout if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2,-1)) * (1/math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T], float('-inf'))\n",
    "            att = F.softmax(att)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        \n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.gelu(self.c_fc(x))\n",
    "        x = self.dropout(self.c_proj(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 组装成block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super.__init__()\n",
    "        self.ln1  = nn.LayerNorm(config.n_embd, bias = config.bias)\n",
    "        self.attn =  SelfAttention(config)\n",
    "        self.ln2  = nn.LayerNorm(config.n_embd, bias = config.bias)\n",
    "        self.mlp  = MLP(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attn(self.ln1(x))  # 残差连接\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 组装成GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "                drop = nn.Dropout(config.dropout)\n",
    "                h = nn.Modulelist([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f = nn.LayerNorm(config.n_embd, bias=config.bias),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)  # 对内部所有子module使用_init_weights函数\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std = 0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "        \n",
    "    def forward(self, idx: torch.Tensor, targets: torch.Tensor = None) :\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = nn.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.ptd(pos)\n",
    "        x = self.transformer.drop(tok_emb+pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cros_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:,[-1],:])\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [1], target: [3])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m pred_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m pred_3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0.3\u001b[39m,\u001b[38;5;241m0.6\u001b[39m,\u001b[38;5;241m0.1\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(F\u001b[38;5;241m.\u001b[39mcross_entropy(real, pred_2))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(F\u001b[38;5;241m.\u001b[39mcross_entropy(real, pred_3))\n",
      "File \u001b[0;32m~/miniconda3/envs/dendro/lib/python3.8/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch (got input: [1], target: [3])"
     ]
    }
   ],
   "source": [
    "real = torch.Tensor(1)\n",
    "pred_1 = torch.Tensor([1,0,0])\n",
    "pred_2 = torch.Tensor([0,1,0])\n",
    "pred_3 = torch.Tensor([0.3,0.6,0.1])\n",
    "print(F.cross_entropy(real, pred_1))\n",
    "print(F.cross_entropy(real, pred_2))\n",
    "print(F.cross_entropy(real, pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l o w </w>', 'l o w e r </w>', 'n e w e s t </w>', 'w i d e s t </w>', 'n e w e s t </w>', 'w i d e s t </w>', 'w i d e s t </w>', 'w i d e s t </w>', 'n i c e </w>']\n",
      "es\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w es t </w>', 'w i d es t </w>', 'n e w es t </w>', 'w i d es t </w>', 'w i d es t </w>', 'w i d es t </w>', 'n i c e </w>']\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w es t </w>', 'w i d es t </w>', 'n e w es t </w>', 'w i d es t </w>', 'w i d es t </w>', 'w i d es t </w>', 'n i c e </w>']\n",
      "est\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est </w>', 'w i d est </w>', 'n e w est </w>', 'w i d est </w>', 'w i d est </w>', 'w i d est </w>', 'n i c e </w>']\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est </w>', 'w i d est </w>', 'n e w est </w>', 'w i d est </w>', 'w i d est </w>', 'w i d est </w>', 'n i c e </w>']\n",
      "est</w>\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est</w>', 'w i d est</w>', 'n e w est</w>', 'w i d est</w>', 'w i d est</w>', 'w i d est</w>', 'n i c e </w>']\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est</w>', 'w i d est</w>', 'n e w est</w>', 'w i d est</w>', 'w i d est</w>', 'w i d est</w>', 'n i c e </w>']\n",
      "wi\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est</w>', 'wi d est</w>', 'n e w est</w>', 'wi d est</w>', 'wi d est</w>', 'wi d est</w>', 'n i c e </w>']\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est</w>', 'wi d est</w>', 'n e w est</w>', 'wi d est</w>', 'wi d est</w>', 'wi d est</w>', 'n i c e </w>']\n",
      "wid\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est</w>', 'wid est</w>', 'n e w est</w>', 'wid est</w>', 'wid est</w>', 'wid est</w>', 'n i c e </w>']\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est</w>', 'wid est</w>', 'n e w est</w>', 'wid est</w>', 'wid est</w>', 'wid est</w>', 'n i c e </w>']\n",
      "widest</w>\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est</w>', 'widest</w>', 'n e w est</w>', 'widest</w>', 'widest</w>', 'widest</w>', 'n i c e </w>']\n",
      "['l o w </w>', 'l o w e r </w>', 'n e w est</w>', 'widest</w>', 'n e w est</w>', 'widest</w>', 'widest</w>', 'widest</w>', 'n i c e </w>']\n",
      "lo\n",
      "['lo w </w>', 'lo w e r </w>', 'n e w est</w>', 'widest</w>', 'n e w est</w>', 'widest</w>', 'widest</w>', 'widest</w>', 'n i c e </w>']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "corpus='''low\n",
    "lower\n",
    "newest\n",
    "widest\n",
    "newest\n",
    "widest\n",
    "widest\n",
    "widest\n",
    "nice'''\n",
    "\n",
    "import regex as re\n",
    "# corpus=corpus.split('\\n')\n",
    "VOVAB_LENGTH=10\n",
    "# corpus_char_counter=Counter(''.join((corpus)))\n",
    "# print(dict(corpus_char_counter))\n",
    "\n",
    "def get_status(corpus):\n",
    "    # 统计相邻元素 XY出现的频率\n",
    "    #  找出最大者\n",
    "    merge_chars=[]\n",
    "    for item in corpus:\n",
    "        char_list=item.split(' ')\n",
    "        for i in range(len(char_list)-1):\n",
    "            \n",
    "            merge_chars.append(''.join(char_list[i:i+2]))\n",
    "            \n",
    "    chars_count=Counter(merge_chars)\n",
    "    most_common=chars_count.most_common(1)\n",
    "    return most_common[0][0]\n",
    "def merge_chars(corpus,chars_most_common):\n",
    "    # 和并上一步得到的出现频率最大元素\n",
    "    for idx,item in enumerate(corpus):\n",
    "        _=re.sub('\\s*'.join(chars_most_common),chars_most_common,item)\n",
    "        corpus[idx]=_\n",
    "    return corpus    \n",
    "def init(words):\n",
    "    for idx,word in enumerate((words)):\n",
    "        words[idx]=' '.join(list(word))+' </w>'\n",
    "    return words\n",
    "words=corpus.split('\\n')\n",
    "corpus=init((words))\n",
    "\n",
    "\n",
    "while len(set(' '.join(corpus).split(' ')))>VOVAB_LENGTH:\n",
    "    print(corpus)\n",
    "    most_common=get_status(corpus)\n",
    "    print(most_common)\n",
    "\n",
    "    corpus=merge_chars(corpus,most_common)\n",
    "    print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low',\n",
       " 'lower',\n",
       " 'newest',\n",
       " 'widest',\n",
       " 'newest',\n",
       " 'widest',\n",
       " 'widest',\n",
       " 'widest',\n",
       " 'nice']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus='''low\n",
    "lower\n",
    "newest\n",
    "widest\n",
    "newest\n",
    "widest\n",
    "widest\n",
    "widest\n",
    "nice'''\n",
    "\n",
    "words=corpus.split('\\n')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', 'c']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ab c'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dendro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
